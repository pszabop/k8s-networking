# k8s networking examples
in this repo I show some examples on how to use Multus to do interesting things with k8s networking, very simply.

## Multi-cloud `IPVlan` networking
I really admire the work that [Lyft](https://github.com/lyft/cni-ipvlan-vpc-k8s) did with implementing `IPVlan` on AWS.  
It allows stateless high performance networking for k8s in AWS by allowing your pods to use cloud-provider-native
IP addresses directly (i.e. your VPC address space). This is particularly critical if you have any pods
exposed to the outside world, as you cannot control the amount of state generated by TCP.  
However Lyft's implemention has the following issues:

1. Lyft's implementation only works with AWS.   You may want to deploy your k8s containers to other clouds.  Then (for example)
   you'd have to use [Azure's](https://github.com/Azure/azure-container-networking) version of Lyft's implementation.  But
   your pod networking internals don't change across clouds, so why should you have to swap out the entire stack?  It'd
   be a maintenance and bug-prone headache to do that.
1. Lyft's implementation is pretty complicated code.  So is Azure's implementation.  And so on...
1. Flannel is already the most cloud portable k8s network stack.  Why not just use flannel as a base and extend from there?
1. Not every container needs high speed network access.  If you are extending your on-prem address space into
   the public cloud, you may only get a /24 for your k8s needs, which means you are wasting valuable addresses for 
   the pods that don't need high performance networking.

The solution may be the following:

1. Run multus so you pods can have multiple networks in your pods.
1. Use flannel as your base network, so all pods can talk to each other on a full mesh control plane.  Flannel
   also takes care of masquerading IP addresses to talk to the outside world if needed.  (e.g. AWS needs this).
   Flannel is the most cloud portable k8s network stack available and it's widely used.
1. For pods that need high performance stateless network access using cloud-provider-native IP addresses,
   use Multus to add an `IPVlan` CNI plugin.

The code in the "cloud-native-addr-ipvlan" does this with some very simple Multus configurations.

### Current issues with the example
A snafu with `IPVlan` is that the k8s host cannot talk to its own pods due to funkiness with how
Linux routes local traffic. The [solution](https://github.com/containernetworking/cni/blob/6737bc8207fd58727a46bba2cfb74f4e9391ad4f/Documentation/ipvlan.md) 
is to use a ptp interface for that purpose.  Currently this is a hack that requires a route to be added, but soon I will 
modify the `IPVlan` CNI plugin to do this automatically for you the same idea Lyft used, a numberless ptp interface.

You must manually tie a route to the IPVlan via the PTP address of the pod:

`ip route add <ipvlan-addr> via <ptp-addr>`

This is easy to automate and the numberless ptp CNI implementation will (eventually) do this for you.

### IPAM is your problem still
Yes I'm completely ignoring IPAM.  IPAM is going to be a cloud-provider specific implementation in any event,
you can't avoid that.   The problem with all the implementations I've seen is security - every k8s node is given
 permission to access the cloud provider's API.  Much better security would be to only allow the k8s master
to do this.  That may be something I tackle in the future.


### Testing done so far
This system works on Virtual Box.  I used `kadm` to set up k8s.  Note that you have to delete `kadm`'s CNI config 
from `/etc/cni/net.d/` before running multus.  Other than that the [default Multus setup](https://github.com/intel/multus-cni/blob/release-v3/doc/quickstart.md) 
in their example works just fine.

A pod's routes look like this, and you can see Flannel is the default route, which is what you want
for IP masquerading:   cloud-provider-native IP addresses will go over the IPVlan network
which means no state tracking or SNAT-ing by flannel.

```
$ k exec -it samplepod -- ip route
default via 10.244.0.1 dev eth0 		                  # Flannel is the default route
10.1.1.0/24 via 10.1.1.1 dev net2  src 10.1.1.3                   # ptp link for local traffic
10.1.1.1 dev net2  scope link  src 10.1.1.3                       # ptp link for local traffic
10.5.2.0/24 dev net1  proto kernel  scope link  src 10.5.2.203    # cloud-provider-native IP addresses using IPVlan
10.244.0.0/24 dev eth0  proto kernel  scope link  src 10.244.0.57 # Flannel pod network
10.244.0.0/16 via 10.244.0.1 dev eth0                             # Flannel pod network
```

Here's the route I had to manually add on the host for this pod.
```
sudo ip route add 10.5.2.203 via 10.1.1.3
```

### Testing that needs to be done
1. Test on AWS
    1. Verify VPC-VPC connections don't get SNAT-ed by Flannel on outbound connections from a pod.
    1. Verify VPC-Outside connections DO get SNAT-ed to an interface's primary IP by Flannel on outbound connections. (this must work or you must pay for a VPC gateway)
1. Make sure this works on Azure
1. Make sure this works on Google Cloud
