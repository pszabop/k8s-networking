# k8s networking examples
in this repo I show some examples on how to use Multus to do interesting things with k8s networking, very simply.

## Multi-cloud `IPVlan` networking
I really admire the work that [Lyft](https://github.com/lyft/cni-ipvlan-vpc-k8s) did with 
implementing `IPVlan` on AWS.  It allows stateless high performance networking for k8s in AWS by 
allowing your pods to use cloud-provider-native (VPC) IP addresses directly (i.e. your VPC address 
space). This is particularly critical if you have any pods exposed to the outside world, as you 
cannot control the amount of state generated by TCP.  However Lyft's implemention has the 
following issues:

1. Lyft's implementation only works with AWS. You may want to deploy your k8s containers to other 
   clouds.  Then (for example) you'd have to use 
   [Azure's](https://github.com/Azure/azure-container-networking) version of Lyft's 
   implementation. But your pod networking internals don't change across clouds, so why should 
   you have to swap out the entire stack?  It'd be a maintenance and bug-prone headache to do that.
1. Lyft's implementation is pretty complicated code.  So is Azure's implementation.  And so on...
1. Flannel is already the most cloud portable k8s network stack.  Why not just use flannel as a 
   base and extend from there?
1. Not every container needs high speed network access.  If you are extending your on-prem address 
   space into the public cloud, you may only get a /24 for your k8s needs, which means you are 
   wasting valuable addresses for the pods that don't need high performance networking.

The solution may be the following:

1. Run Multus so you pods can have multiple networks in your pods.
1. Use flannel as your base network, so all pods can talk to each other and to the nodes on a full 
   mesh control plane.  Flannel also takes care of masquerading IP addresses to talk to the outside 
   world if needed.  (e.g. AWS needs this).  Flannel is the most cloud portable k8s network stack 
   available and it's widely used.
1. For pods that need high performance stateless network access using cloud-provider-native IP 
   addresses, use Multus to add an `IPVlan` CNI plugin.

The code in the "cloud-native-addr-ipvlan" does this with some very simple Multus configurations.


### IPAM is your problem still
Yes I'm completely ignoring IPAM.  IPAM is going to be a cloud-provider specific implementation in any event,
you can't avoid that.   The problem with all the implementations I've seen is security - every k8s node is given
 permission to access the cloud provider's API.  Much better security would be to only allow the k8s master
to do this.  That may be something I tackle in the future.

Note that the Lyft implementation uses separate plugins for IPAM and it may be possible
to reuse their IPAM implementation.  However, they have a lot of "set it and check next Tuesday"
dependencies between their plugins, so possibly not.  THey also have the security issue
mentioned above.

### Testing done so far
This system works on Virtual Box.  I used `kadm` to set up k8s.  Note that you have to delete 
`kadm`'s CNI config from `/etc/cni/net.d/` before running multus.  Other than that the 
[default Multus setup](https://github.com/intel/multus-cni/blob/release-v3/doc/quickstart.md) 
in their example works just fine.

A pod's routes look like this, and you can see Flannel is the default route, which is what you want
for IP masquerading, talking to the host node, and talking to control plane components such as 
kube-api. Cloud-provider-native IP addresses will go over the IPVlan network which means no state tracking 
or SNAT-ing by flannel.  This means Apache Bench will no longer be able to trivially take
down your nodes by effectively spamming the connection tracking table.

```
$ k exec -it samplepod -- ip route
default via 10.244.0.1 dev eth0 		                  # Flannel is the default route
10.5.2.0/24 dev net1  proto kernel  scope link  src 10.5.2.203    # cloud-provider-native IP addresses using IPVlan
10.244.0.0/24 dev eth0  proto kernel  scope link  src 10.244.0.57 # Flannel pod network
10.244.0.0/16 via 10.244.0.1 dev eth0                             # Flannel pod network
```

#### Specific tests - Virtual Box
1. Ensure pods on the same node can ping each other over the IPVlan network. *Works.*
1. Ensure pods on the different nodes can ping each other over the IPVlan network. *Works.*
1. Ensure remote VMs can access the echoheaders test pod and ensure source IP address is intact.  *Works.*
1. Ensure two way control plane access over flannel. *Works*
1. Ensure access to the outside world from the samplepod with IP masquerade provided by Flannel.  *Works.*
1. Ensure when the IPVlan network is accessed there are no IPTables conn tracking involved in an
   of the above tets.  *Works.*

### Testing that needs to be done
1. Test on AWS
    1. Verify VPC-VPC connections don't get SNAT-ed by Flannel on outbound connections from a pod.
    1. Verify VPC-Outside connections DO get SNAT-ed to an interface's primary IP by Flannel on outbound connections. (this must work or you must pay for a VPC gateway)
    1. See what impact VPC subnets has on the system.
1. Make sure this works on Azure
1. Make sure this works on Google Cloud

### Current issues with the example
#### Local node access to its pods on the IPVlan Network
A snafu with `IPVlan` is that the k8s host cannot talk to its own pods on that network due to 
funkiness with how Linux routes local traffic in relation to `IPVlan`. 
The [solution proposed by the IPVlan CNI plugin](https://github.com/containernetworking/cni/blob/6737bc8207fd58727a46bba2cfb74f4e9391ad4f/Documentation/ipvlan.md) 
is to set up an ptp interface for that purpose.  Lyft uses a numberless PTP
and routes all traffic through it.  This seems like a lot of complexity when there's 
already a flannel network for the pod to accomplish control plane communications.

There's no problem with other pods talking on the IPVlan network for on-node or off-node
pod-pod traffic using cloud-provider-native IP addresses, so I'm going to assume the complexity
of fixing this is not worth it.  File an issue if you can think of a use case that requires
local node access specifically the Pod's IPVlan IP address.

#### VPC subnet routes
The Lyft implementation includes giving all routes that the host is aware of to the pod's
network (but on the host side),, which may be required by AWS's VPC subnet architecture.  
However, they do NOT take care of any updates that routing, it only happens at pod creation 
time.   However I will study this issue.  The solution for this setup would be to have a 
host-side agent or a pod sidecard that updates all pods that us IPVlan network, just like 
AWS already provides an agent that updates the host side network.

### Interactions with other kube networking components
#### Kube-Proxy
Kube Proxy is already stateful and low performance, and will continue to work with the 
Flannel network.

If you need high performance proxy (e.g. load balancing) services you want something
that is also on the cloud-native-address network.  The cloud provider's load
balancer would the appropriate solution in that case.

I note that one of the use cases for a high performance k8s network is when
your clients already know how to do their own load balancing (e.g. Redis with
ioredis).  So kube-proxy would not be used in that case anyways.

### NodePorts
One of the reasons one has to use NodePorts in k8s is because you can't address
pods directly from external clients.   Using cloud-provid-native IP addresses
solves this problem so there's no need to use NodePort for that use case.

That being said, if you use NodePorts they will continue to work with the Flannel
Network.  They just won't be as high performance.  THey never were anyways.
